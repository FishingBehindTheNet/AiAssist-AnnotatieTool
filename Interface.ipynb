{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Ai assisted annoteren: de Pijplijn**\n",
    "Dit notebook is gemaakt als een pijplijn voor het annoteren en het maken van modellen voor het gebruik in ai assisted annoteren. Het idee hierbij is niet dat een complete dataset geannoteerd word voordat het eerste model getraind word maar in plaats daarvan periodiek een model getraind word wat daarna gebruikt kan worden om te helpen bij het annoteren van de rest van de dataset. Momenteel werkt de code voor .PNG .JPEG en .JPG bestanden. Op verzoek kan deze of andere functies aangepast of uitgebreid worden. Voor vragen, verzoeken of bugs kun je contact opnemen via de github issue tracker.\n",
    "\n",
    "De tool is gebouwd op python 3.9 en maakt gebruikt van alle bibliotheken in de requirements.txt file. Daarnaast kan [PyTorch](https://pytorch.org/get-started/locally/) gebruikt worden om training op GPU te laten runnen. Hiervoor moet bij de aanmaak van een .venv ***EERST*** de correcte versie van [PyTorch](https://pytorch.org/get-started/locally/) geïnstalleerd worden ***VOOR*** het installeren van de requirements.txt file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Foto segmentatie of herordening**\n",
    "Deze tool is bedoeld als een eerste stap van een project of nieuwe dataset. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De tool bestaat uit twee hoofdcomponenten: herordening en segmentatie. Het herordening tabblad is bedoeld voor situaties waarin de foto's in de dataset al zijn gesegmenteerd en mogelijk al annotaties bevatten. Dit tabblad kan ook worden gebruikt voor datasets waarbij de foto's niet op volledige resolutie worden getraind. Bijvoorbeeld, als je werkt met foto's van wildcamera's waarop slechts enkele dieren per foto zichtbaar zijn. Het is echter belangrijk op te merken dat bij het trainen de pixelgrootte grote impact heeft op de trainingstijd en mogelijk verlaagd moet worden om de trainingstijd en voorspellingstijd te verminderen. Als je dit tabblad gebruikt, worden alleen de foto's en eventuele labels gekopieerd naar de geselecteerde projectmap, zodat ze in de juiste map hiërarchie staan, maar nog steeds op hun oorspronkelijke locatie beschikbaar zijn. De foto's en/of segmenten worden opgeslagen onder \"\\\\{project}\\\\Data\\\\Annoteren\\\\{naam dataset}\\\\images\" de labels worden onder dezelfde locatie opgeslagen echter is hierbij de laatste map \"labels\" in plaats van \"images\"\n",
    "\n",
    "De foto-segmentatiefunctionaliteit is ontworpen om foto's op te delen in gelijke segmenten. Dit helpt zowel het model tijdens het trainingsproces als de gebruiker tijdens het annoteren. Stel je bijvoorbeeld een complexe afbeelding voor, zoals een 'Waar is Waldo'-plaatje. Het wordt moeilijk om de hele afbeelding te annoteren en Waldo te vinden. Door de afbeelding in segmenten op te delen, wordt dit zowel voor het model als voor jou als gebruiker overzichtelijker. Bij het segmenteren kun je twee parameters instellen: de grootte van de segmenten in pixels en de mate van overlap tussen de segmenten. De grootte bepaalt de afmetingen van de segmenten, waarbij de standaardwaarde meestal 640 pixels is, maar deze kan naar wens worden aangepast. De overlap bepaalt hoeveel van het segment overeenkomt met aangrenzende segmenten. Deze functie is handig om ervoor te zorgen dat objecten die zich op de grens van twee segmenten bevinden, volledig worden weergegeven in ten minste één van de segmenten. \n",
    "\n",
    "De naam van de segmenten wordt opgebouwd als volgt:\n",
    "\n",
    "* {Naam originele foto}_{X coördinaat hoek links boven op originele foto in pixels}_{Y coördinaat hoek links boven op originele foto in pixels}.jpg.\n",
    "\n",
    "De segmenten worden opgeslagen onder de volgende locatie:\n",
    "* .\\\\{project}\\\\Data\\\\Annoteren\\\\{naam dataset}\\\\images\\\\\n",
    "\n",
    "veder word er in de {naam dataset} map een lege folder voor labels gemaakt."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15670fdee5c24461ab3bbd342104ce8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(VBox(children=(HBox(children=(VBox(children=(VBox(children=(HTML(value='<b>Selecteer een project…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "812a7960176441eb92ca30cd9e413311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value=\"<h3>Foto's worden gesegmenteerd:</h3>\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf176bb0ad944281a83cb28298f58524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='Progress:', max=13)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418ef52e7b934d25912d8c9dfa71ea05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value=\"\\n            <style>\\n            td:first-child {\\n                text-align: right;\\n         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from Modules.A1_FotoPreparatie import FotoPreparatie\n",
    "FotoPreparatie()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Annoteren**\n",
    "Dit is de interface waarin je daadwerkelijk de annotaties maakt. Je kunt bounding boxes tekenen om objecten op de foto's te markeren. Deze interface is ontworpen om het annotatieproces te vergemakkelijken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bij het opstarten van de interface krijg je eerst 3 schermen voordat je kunt annoteren:\n",
    "*   **Model keuze:** Dit scherm mag je overslaan als je nog geen model hebt. Heb je al wel een model selecteer deze dan om tijdens het annoteren de AI assist te kunnen gebruiken. Deze functie genereerde annotaties met behulp van het model die je daarna allen nog hoeft te verbeteren en op te slaan. Als je deze functie goed gebruikt kan het je een hoop tijd schelen.\n",
    "*   **Folder selectie:** Dit spreekt voor zichzelf.\n",
    "*   **Label Aanmaken:** Hier maak je de lijst mat labels/classen die je kunt selecteren tijdens het annoteren. De labels worden opgeslagen in \"Labels.txt\" in de geselecteerde label folder. Je kunt hier geen labels aanpassen omdat dit problemen kan geven voor annotaties die je eerder gemaakt hebt. Omdat YOLOv8 cijfers in plaats van labels als classes nodig heeft werkt dit document als een woorden boek om labels te kunnen vertalen (2 verwijst naar regel 2, handen,  in Labels.txt). Als het lijstje in dit document een andere volgorde krijgt of labels verdwijnen kan het zijn dat labels niet goed meer te vertalen zijn en zijn alle eerder gemaakt labels nu fout(2 verwijst nu bijvoorbeeld naar voeten omdat regel 2 verwijdert is uit Labels.txt). mocht je toch iets willen aanpassen kun je dit doen door zelf in de verkenner naar dit document te navigeren.\n",
    "*   **Annotatieproces:** Hier begin je met het annoteren van de foto's met behulp van de eerder gedefinieerde labels. Je kunt bounding boxes tekenen rond de objecten op de foto's en de bijbehorende klassen toewijzen aan de bounding box. Na het voltooien van de annotaties worden de labels opgeslagen door op de knop \"Submit\" te klikken. Als je op de knoppen \"Back\" of \"Skip\" klikt, worden de annotaties niet opgeslagen. De interface kan worden bediend met de muis via de verschillende weergegeven knoppen. De kaders van de bounding boxes kunnen worden getekend door te klikken op de positie van een van de toekomstige hoeken van de nieuwe bounding box en deze naar de tegenovergestelde hoek te slepen. Het toewijzen van klassen aan de bounding box kan vooraf worden gedaan door het juiste label onder de foto te selecteren of met de overeenkomstige cijfertoetsen op het toetsenbord. Het aanpassen van de klassen na het maken van de bounding box is ook mogelijk. Selecteer hiervoor het juiste label onder de foto, klik vervolgens op de naam die boven de bounding box wordt weergegeven. Dit kan ook worden gedaan door op de bounding box te klikken, het juiste label te selecteren en vervolgens op de toets <kbd>C</kbd> te drukken.\n",
    "\n",
    "**TIP: Tijdens het annoteren kom je ook foto's/segmenten tegen zonder objecten die je wilt herkennen. Sla genoeg van deze segmenten toch op met <kbd>Submit</kbd> zodat je toekomstige model ook weet wat het niet hoeft te herkennen.**\n",
    "\n",
    "Voor het annoteren is het belangrijk dat je met je projectgroep goed vastlegt wat wel en niet binnen je verschillende classen/labels valt. Stel regels op over wat je wel en niet annoteert en noteer deze regels zodat je er later op terug kan vallen. Gebruik tijdens het annoteren de check later functie zodat je moeilijke gevallen kunt bespreken met je groepje en periodiek kunt controleren of iedereen nog steeds op de zelfde manier annoteerde. Dit is een goede methode om de kwaliteit van de annotaties te waarborgen als er verschillende mensen tegelijk annoteren.\n",
    "\n",
    "Daarnaast word het aanbevolen periodiek een model te trainen met de annotaties die je gemaakt hebt. Dit is geen verplichting maar de kracht van deze tool zit hem juist in de volgende werk loop: annoteer x aantal foto's -> genereer een model -> gebruik model als AI assist -> annoteer x aantal foto's -> genereer een nieuw beter model -> etc.\n",
    "\n",
    "De annotatietool biedt ook handige knoppen om het annotatieproces te beheren:\n",
    "\n",
    "*   <kbd>Back</kbd> Hiermee ga je een foto terug in het annotatieproces. De annotaties worden niet opgeslagen.\n",
    "*   <kbd>Skip</kbd> Hiermee ga je naar de volgende foto zonder de annotaties op te slaan.\n",
    "*   <kbd>Submit</kbd> Hiermee ga je naar de volgende foto en worden de annotaties opgeslagen.\n",
    "*   <kbd>AI Aan/Uit</kbd> <b>(Staat standaard uit)</b> Met deze knop bepaal je of het geselecteerde model wordt gebruikt om annotaties te genereren. Als de AI Assist is ingeschakeld, worden voorspellingen weergegeven op basis van het model. Druk op \"Submit\" om de voorspelling daadwerkelijk te gebruiken.\n",
    "*   <kbd>Genereer annotaties</kbd> Deze knop verwijdert eerder gegenereerde labels en genereert nieuwe voorspellingen. Dit kan handig zijn als de huidige voorspellingen niet overeenkomen met de foto of door een eerdere versie van het model gemaakt zijn. In het laatste geval kan het ook slim zijn de \"Voorspellingen\" folder onder data in je project folder leeg te maken.\n",
    "*   <kbd>Check Later</kbd> Hiermee kun je een kopie van de foto opslaan in de \"check later\" folder in je geselecteerde image folder. Om de foto terug te zien kun je de interface opnieuw opstarten en bij \"Folder selectie\" de check later map als de image folder selecteren.\n",
    "*   <kbd>Verwijder foto</kbd> Verwijdert het segment of de foto uit de geselecteerde Image folder. Eventuele labels worden ook verwijdert tenzij de geselecteerde map de \"Check later\" map is."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bfd23ed668d46f888e8736bb3be145b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h1>Model keuze (Niet verplicht!)'), VBox(children=(HTML(value='<b>Selecteer een pr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from Modules.B1_ModelPicker import Launch\n",
    "\n",
    "#Label widget moet naar externe file\n",
    "Launch()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data voorbereiding**\n",
    "De \"Data Move\" is een voorbereidende stap voor het trainen van een nieuw model. Het is bedoeld om de annotaties die gemaakt zijn te herordenen, zodat ze kunnen worden ingelezen tijdens het trainen. De tool kopieert of verplaatst (afhankelijk van de knop die je kiest \"Verplaats\" of \"Kopieer\") de foto's in de geselecteerde map die bijbehorende annotaties hebben in de geselecteerde label folder. De foto's worden verzameld onder \"/{project folder}/Data/Training en Test set/{Opgegeven naam dataset}\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Bij het trainen van een nieuw model voor machine learning en beeldherkenning is het belangrijk om rekening te houden met concepten zoals overfitting, validatie- en testsets.\n",
    "\n",
    "Overfitting treedt op wanneer een model te specifiek wordt getraind op de beschikbare data en daardoor moeite heeft om goed te presteren op nieuwe, onbekende data. Dit gebeurt wanneer het model te veel aandacht besteedt aan de details en ruis in de trainingsdata, waardoor het niet goed kan generaliseren naar nieuwe voorbeelden. Een analogie hiervoor is bijvoorbeeld het studeren voor een Frans toets met flipkaartjes. Als je te veel studeert met de kaartjes, begin je de antwoorden te linken aan externe factoren, zoals een specifieke vouw in de hoek van het kaartje. Dit is echter niet nuttig tijdens de toets en leidt tot slechtere prestaties.\n",
    "\n",
    "Om overfitting te voorkomen, is het noodzakelijk om de beschikbare data op te splitsen in trainings-, validatie- en testsets. De trainings set bestaat uit gelabelde voorbeelden die worden gebruikt om het model te trainen. Het is belangrijk om een voldoende grote en representatieve trainings set te hebben, zodat het model patronen en kenmerken kan leren en generaliseren. Terugkomend op het voorbeeld van de Frans toets, de trainings set bestaat uit de relevante theorie en voorbeelden die je gebruikt om te studeren. Als je niet genoeg trainingsdata hebt, loop je het risico dat je niet genoeg kennis en ervaring hebt om goed te presteren op de toets.\n",
    "\n",
    "De validatie set wordt gebruikt tijdens het trainingsproces om de prestaties van het model te meten en eventuele problemen zoals overfitting te detecteren. Het is een apart deel van de data dat niet wordt gebruikt om het model te trainen, maar om te evalueren hoe goed het model generaliseert naar nieuwe, ongeziene data. Hierdoor kun je aanpassingen maken aan het model en de trainingsparameters om de prestaties te verbeteren. Het percentage van de foto's dat moet worden gebruikt als validatie set moet meer dan 0% zijn. Terugkomend op het Frans toetsvoorbeeld, de validatie set kun je zien als een oefentoets die je maakt om te kijken of je het begrijpt. Als je te veel oefentoetsen maakt, besteed je mogelijk te weinig tijd aan het studeren, terwijl te weinig oefentoetsen je de kans ontneemt om moeilijke vragen te oefenen en te bepalen of je de stof echt beheerst.\n",
    "\n",
    "De testset is een aparte dataset die pas aan het einde van het trainingsproces wordt gebruikt om de uiteindelijke prestaties van het getrainde model te evalueren. Het is belangrijk om de testset niet te gebruiken tijdens het trainen of valideren, zodat de resultaten objectief blijven en de trainingsbeslissingen niet beïnvloeden. Deze data komt pas aan het einde van je project van pas wanneer je resultaten genereert voor je verslag of een keuze moet maken tussen twee modellen die je gemaakt hebt. Terugkomend op het Frans toetsvoorbeeld, de testset is als de oefentoets die je docent de les van tevoren geeft om te oefenen voor de daadwerkelijke toets. Het is de laatste controle om te bepalen of je waarschijnlijk zult slagen of dat je nog een stukje hebt gemist en nog wat langer moet studeren. Het is een handig hulpmiddel om eventuele zwakke punten op te sporen, maar als je te vaak dezelfde oefentoetsen maakt, loop je het risico dat je eigenlijk aan het studeren bent voor de oefentoetsvragen en niet echt de taal leert.\n",
    "\n",
    "Door het gebruik van trainings-, validatie- en testsets kun je overfitting voorkomen, het model afstemmen en de prestaties objectief beoordelen. Hoewel in het gebruik van deze tool het mogelijk niet direct nodig is om een testset te specificeren, omdat je waarschijnlijk veel nieuwe modellen maakt met als enige onderscheid: meer data, is het nog steeds essentieel om een voldoende percentage van de foto's te reserveren als validatie set. YOLOv8 gebruikt deze data namelijk tijdens het trainen en zal zonder deze data dan ook een error geven.\n",
    "\n",
    "Na het verplaatsen van de foto's en bijbehorende labels wordt er een configuratiebestand aangemaakt in de {data naam} folder. Dit bestand geeft aan waar de data op het apparaat te vinden is en moet worden aangepast wanneer de data naar een nieuwe locatie op dezelfde of een andere computer wordt verplaatst. Dit bestand heb je nodig voor het instellen van het trainings process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f8175c8bb549b59ffb5e2d46ee3fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h1>Data voorbereiding'), HBox(children=(VBox(children=(HTML(value='<h2>Huidige Loc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from Modules.C1_PreTrain import AnnotationMove\n",
    "\n",
    "AnnotationMove()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Train**\n",
    "\n",
    "Deze interface is bedoeld voor het trainen van een beeldherkenningsmodel. De interface heeft 3 tabbladen. Het eerste tabblad is bedoeld voor verder trainen met een bestaand model. Dit is handig wanneer je al een model hebt getraind, dit model hebt gebruikt om meer data te verzamelen, en nu een beter model wilt maken met dezelfde labels. Je kunt bijvoorbeeld de precisie en recall verbeteren of afwijkingen in de data corrigeren door nieuwe data toe te voegen zonder de afwijkingen.\n",
    "Het tweede tabblad is bedoeld voor het trainen van volledig nieuwe modellen. Dit is handig voor het eerste model dat wordt gemaakt tijdens een project of wanneer je extra labels aan een model wilt toevoegen.\n",
    "Het derde tabblad is bedoeld voor het voortzetten van een training, bijvoorbeeld wanneer je een training hebt gestart op je laptop maar deze niet was voltooid voordat je naar huis moest. In dit geval hoef je alleen maar het model te selecteren. Deze optie is niet mogelijk wanneer trainings- of validatiedata is verplaatst."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"color:red;\">*Disclaimer: Bij het trainen van een model zijn er meer instellingen beschikbaar dan die worden aangeboden in deze interface. Het doel van de interface is niet om het beste model te maken, maar om een snelle pijplijn te bieden voor het verzamelen, trainen en verbeteren van een model om het annotatieproces te versnellen.*</code>\n",
    "\n",
    "Tijdens het trainen vanaf de grond af aan of vanaf een ander model worden verschillende instellingen aangeboden. Hieronder volgt een beknopte uitleg van de betekenis en invloed van verschillende instellingen:\n",
    "\n",
    "* Modelkeuze:\n",
    "    * De keuze van het model heeft veel invloed wanneer je traint vanaf een bestaand model. Het geselecteerde model heeft al kennis van je labels en heeft ook al een keuze gemaakt voor een YOLO-model. Een model dat is gebaseerd op veel data heeft al een redelijk beeld en zal mogelijk minder effect zien van meer van dezelfde data, waardoor je misschien meer epochs moet doorlopen voordat je verbeteringen ziet. Het voordeel hiervan is echter dat het over het algemeen betere resultaten oplevert, omdat het model al een goed beeld heeft en alleen nog op nuances hoeft te letten.\n",
    "    * Bij het trainen van een compleet nieuw model heeft de modelkeuze vooral invloed op de trainingsduur en de maximale precisie en recall die je kunt behalen. Een kleiner model zorgt voor een snellere training en snellere voorspellingen na het trainen. YOLOv8 biedt modellen aan in verschillende maten, variërend van nano tot Xtra large, waarbij nano het snelste en lichtste model is, en Xtra large het grootste en meest complexe model. Het is aan te raden om klein te beginnen en als je merkt dat de resultaten na het trainen met voldoende data niet hoog genoeg zijn, kun je experimenteren met grotere modellen. Houd er echter rekening mee dat grotere modellen zowel tijdens het trainen als tijdens het gebruik meer computerbronnen vereisen en mogelijk langere verwerkingstijden hebben.\n",
    "* Configuration file\n",
    "    * Dit bestand geeft aan waar je foto's en labels zich bevinden en welke labels zijn gebruikt tijdens het annotatieproces. Het configuratiebestand wordt aangemaakt bij het verplaatsen van de data met behulp van de \"Data Move\", zoals hierboven beschreven, en hoeft verder niet aangepast te worden, tenzij je de mappen met data verplaatst.\n",
    "* Project folder\n",
    "    * Hier geef je aan waar je het nieuwe model wilt opslaan. Het trainingsproces maakt een folder aan waarin aan het einde van de training [grafieken](https://learnopencv.com/mean-average-precision-map-object-detection-model-evaluation-metric/) worden gegenereerd die de trainingsvoortgang en de prestaties van het nieuwe model weergeven. Binnen het geselecteerde project wordt ook een map voor modellen aangemaakt, waarin de nieuwe modellen worden gekopieerd en omgezet naar een .onnx-model (.pt voor training en .onnx voor snellere voorspellingen op de CPU). Het geselecteerde project hoeft niet hetzelfde te zijn als het project waarin je data is opgeslagen.\n",
    "* Model naam\n",
    "    * Hiermee geef je aan hoe de folder met de hierboven genoemde grafieken en het nieuwe modellen worden genoemd.\n",
    "* Nummer of epochs:\n",
    "    * Het aantal epochs geeft aan hoe vaak je het model over de gehele dataset wilt laten trainen. Meer epochs betekenen over het algemeen betere resultaten, maar te veel epochs kunnen leiden tot overfitting. Het juiste aantal epochs hangt onder andere af van de complexiteit van je data, de hoeveelheid data, het aantal klassen dat je hebt en de grootte van je batch. Dit is een waarde die je alleen kunt vinden door te experimenteren. Op basis van de hoeveelheid data kun je een ruwe schatting maken en je model trainen. Daarna kun je naar de geproduceerde grafieken kijken, zoals de loss-factoren. Over het algemeen wil je zien dat de grafiek afvlakt en niet verder stijgt. Voor meer informatie kun je zoeken naar 'object detection loss' en 'overfitting'.\n",
    "* Batchgrootte:\n",
    "    * De batchgrootte geeft aan hoeveel foto's of segmenten het model tegelijkertijd bekijkt tijdens het trainen. Hoe hoog deze waarde kan zijn, hangt af van de beschikbare geheugenruimte en de omvang van je data. Dit is een waarde die ook moet worden getest en geoptimaliseerd. Probeer de batchgrootte zo groot mogelijk in te stellen zonder dat de training crasht vanwege geheugenproblemen. Over het algemeen zorgt een grotere batchgrootte voor een beter model in minder stappen. Als de loss-grafiek na het trainen grote pieken en dalen vertoont in plaats van een redelijk constante lijn, kan dit een indicatie zijn van een te kleine batchgrootte, wat de prestaties van het model permanent kan belemmeren.\n",
    "* Foto size\n",
    "    * Hiermee geef je aan hoe groot je segmenten (images) zijn. Over het algemeen wordt aanbevolen om deze waarde gelijk te houden aan de grootte van je segmenten. Het is echter ook mogelijk om deze waarde hoger of lager te maken (waarbij YOLOv8 de resolutie van de foto aanpast). Het voordeel hiervan kan zijn dat de belasting van je computer wordt verminderd en de batchgrootte kan worden verhoogd. Houd er echter rekening mee dat je hiermee ook scherpte en detail in je foto's verliest, dus gebruik deze optie met mate. Foto's met andere afmetingen worden passend gemaakt binnen de opgegeven vierkante afmetingen door extra zwarte randen boven of links van de foto te plaatsen om de 1:1-verhouding te behouden.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a1b811c116a4462bbbffe3a227c709c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(VBox(children=(FileChooser(path='C:\\Users\\Sjoer\\AAA-Python\\WWEL-Sjoerd\\AiAssist-AnnotatieTool', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from Modules.D1_ModelTrain import train\n",
    "train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Valideren**\n",
    "\n",
    "Deze laatste stap is bedoelt voor het verzamelen van data over de prestatie van je model(len). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tijdens het trainen zijn deze [grafieken](https://learnopencv.com/mean-average-precision-map-object-detection-model-evaluation-metric/) ook al gegenereerd en heb je deze kunnen gebruiken om je train te evalueren. De tool hieronder is bedoelt om deze grafieken te kunnen genereren op eenzelfde dataset met verschillende modellen en om gebruik te kunnen maken van de test set die je eerder gemaakt hebt. Om een dataset te gebruiken heb je wederom het configuratiebestand van de datavoorbereiding nodig. Door op de Test set knop te drukken kun je kiezen om de grafieken te genereren met je de test set of met de validatie set waarnaar verwezen word in het configuratiebestand. Daarnaast kun je de confidence opgeven, dit bepaalt hoe zeker het model moet zijn voordat het een bounding box tekent. De resultaten worden opgeslagen onder \"/{project folder}/Model Resultaten/{Model naam}\" hier worden de grafieken opgeslagen onder een folder met de geselecteerde instellingen zodat er duidelijk onderscheid gemaakt kan worden tussen de verschillende validaties."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbad6ebe0d364bdb8e24c3e80a71444d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>Model validatie</h3><hr>'), HBox(children=(VBox(children=(FileChooser(path='C:\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from Modules.E1_ModelValidatie import ModelValidation\n",
    "\n",
    "ModelValidation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Well-Anotate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
